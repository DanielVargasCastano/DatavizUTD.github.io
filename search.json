[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Davaquarto",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sub1.html#return-home",
    "href": "sub1.html#return-home",
    "title": "Test1page",
    "section": "return home",
    "text": "return home"
  },
  {
    "objectID": "sub1.html",
    "href": "sub1.html",
    "title": "Test1page",
    "section": "",
    "text": "“return”"
  },
  {
    "objectID": "sub1.html#return",
    "href": "sub1.html#return",
    "title": "Test1page",
    "section": "#return",
    "text": "#return"
  },
  {
    "objectID": "assigment1.html",
    "href": "assigment1.html",
    "title": "assigment1",
    "section": "",
    "text": "Atomic Orange Tree\n\n\n\n\n\nHouse Proceduraly Generated. Source:TheRookies.co\n\n\n\n\n\nSpelunky map Generation. Source TinySubversions.com\n\n\n\n\n\nMinecraft map Generation. Source: https://ucmgamestudies2017.wordpress.com\n\n\n\n\n\nChart Critique: Cars Satisfaction Score. Source: Consumer Reports\n\n\nThe chart is basically a table with a colorful line for the satisfaction score. Bold letters make it clear which category is each column, and the bold number in the rank column shows the rank of each car make. All of the car brands are underlined, which does not add much to the design. The colors on the Satisfaction score clearly depict which car brand has higher consumer satisfaction with a stoplight color pallet. Then even more detail is added with the percentage of owners who would buy the same brand again. There is a problem when there are ties as it is unclear of why one brand would be preferred over the other. The rank should stay constant in those cases and skip the ranks for the next one. Otherwise, the rank column does not give much information.\nreturn"
  },
  {
    "objectID": "note1.html",
    "href": "note1.html",
    "title": "Tufte: Future of Data Analsis",
    "section": "",
    "text": "Dr. Edward Tufte’s keynote at the 2016 Microsoft Machine Learning & Data Science Summit highlighted the critical role of data visualization in transforming raw data into meaningful narratives. He emphasized that effective visualizations lead to “visible certainty,” where data is clearly communicated, enabling informed decision-making. Tufte argued that data scientists must go beyond making data visually appealing and focus on supporting reasoning and understanding through their designs. He used historical examples like Galileo’s use of the telescope to illustrate how visual aids have long been essential in advancing understanding. For modern data scientists, Tufte stressed the importance of creating visualizations that simplify and clarify complex data. He warned against the dangers of confirmation bias and distorting data to fit preconceived notions, emphasizing the need for accuracy and integrity in data analysis. Tufte advocated for visual designs that highlight causal relationships, facilitate smart comparisons, and avoid unnecessary clutter. He emphasized that the true purpose of data visualization is to aid in thinking and reasoning, not just to present information.\nLooking to the future, Tufte noted the potential of new technologies like high-resolution screens and 3D visualizations to enhance data presentations. He praised the clarity of Swiss maps and the storytelling techniques of Pixar as models for content-driven design.\nHe also discussed the current crisis in data analysis, where many studies suffer from overfitting and flawed methodologies, citing the failure of the Google Flu model as an example. Tufte urged data scientists to adopt rigorous, transparent practices, such as pre-specified research designs and independent replication, to ensure accuracy.\nIn closing, Tufte called for a higher standard of analytical rigor and integrity in data science, challenging practitioners to prioritize truth and clarity in their work."
  },
  {
    "objectID": "note2.html",
    "href": "note2.html",
    "title": "Pitfalls of Big Data",
    "section": "",
    "text": "Big data analytics offer immense opportunities, but they are accompanied by significant challenges that can lead to errors if not properly managed. A notable example of this is the Google Flu Trends (GFT), which inaccurately predicted flu trends due to several key pitfalls. One major challenge in big data analytics is the assumption that sheer volume of data can replace traditional, well-validated methods. GFT’s approach to predicting flu trends relied heavily on large amounts of search data, underestimating the importance of scientific rigor in data measurement and validity. This overconfidence in big data led to the oversight of critical issues like construct validity and reliability, resulting in inaccurate predictions.\nIn addition to this overconfidence, changes in Google’s search algorithms contributed to the inaccuracies. Google regularly updates its search algorithm for commercial optimization, and these adjustments, while beneficial for user experience, altered the underlying data GFT depended on. As user behavior and search algorithms shifted, GFT’s flu predictions became less reliable. The GFT parable emphasizes the need to account for such dynamic changes in data sources, as failing to understand these shifts can lead to significant errors.\nA related issue was overfitting, where the model fit the training data too perfectly, capturing irrelevant patterns or noise instead of the underlying trends. In GFT’s case, the system initially matched 50 million search terms to a small set of CDC data points. This method increased the chances of finding unrelated search terms that coincidentally aligned with flu patterns. When real-world conditions changed, such as during the non-seasonal H1N1 pandemic, the model failed because it had been tailored too closely to past flu patterns. Additionally, the algorithm’s reliance on a large number of variables, known as overparameterization, made it more prone to overfitting. Although the model discarded some irrelevant seasonal search terms, this wasn’t enough to prevent it from being overly sensitive to coincidental patterns, such as searches for unrelated topics like “high school basketball” that happened to coincide with flu season.\nThe GFT example highlights the importance of integrating big data with traditional analytical approaches. Overreliance on data quantity, without sufficient attention to methodology, can lead to models that misinterpret trends and produce misleading results. To avoid these pitfalls, big data analytics must strike a balance between leveraging large data sets and applying rigorous, time-tested methods of analysis."
  },
  {
    "objectID": "note3.html",
    "href": "note3.html",
    "title": "Tufte vs Wickham",
    "section": "",
    "text": "Edward Tufte and Hadley Wickham are two influential figures in the field of data visualization, each contributing in distinct ways to how we represent and understand data. While Tufte is known for his philosophy and principles on the aesthetics and ethics of information design, Wickham is celebrated for his practical contributions to the tools and techniques used to create visualizations, particularly within the R programming language. Their work, while complementary, reflects different approaches to data communication: one emphasizing the principles of clarity and simplicity, and the other focusing on the mechanics and flexibility of data manipulation and visualization.\nTufte has written seminal works like The Visual Display of Quantitative Information, where he outlines his core principles: clarity, precision, and efficiency. He emphasizes reducing “chartjunk,” unnecessary or distracting elements in visualizations, in favor of minimalism and clarity. Tufte’s design philosophy insists on maximizing the data-ink ratio, meaning every element on the page should convey meaningful information, leaving no room for decorative excess. His well-known examples, such as Charles Minard’s 1869 depiction of Napoleon’s Russian campaign, exemplify how complex data can be communicated with both simplicity and impact. Tufte’s work has deeply influenced the ethics of data visualization, advocating for transparency and accuracy in conveying information to avoid misleading or confusing the viewer.\nHadley Wickham, on the other hand, is known for his contributions to the technical side of data visualization and analysis. As a statistician and software developer, he created tools like ggplot2, a widely-used R package based on the Grammar of Graphics, which allows users to systematically build data visualizations. Wickham’s work enables the flexible creation of complex, layered plots, giving users granular control over every aspect of the visual representation. His focus is on the reproducibility and modularity of visualization, making it easier for analysts and researchers to create and iterate on their work programmatically. Wickham has also developed other tools in the R ecosystem (e.g., dplyr, tidyr) that facilitate data cleaning, manipulation, and preparation, all critical steps before visualization. Unlike Tufte, whose emphasis is on final presentation, Wickham provides tools that empower users throughout the entire data workflow.\nThe contrast between Tufte and Wickham is not just philosophical but also practical. Tufte focuses on the final output—the polished, minimalist product meant for conveying complex information with clarity to an audience. He is more concerned with design aesthetics and cognitive clarity. Wickham, however, is more focused on how data gets transformed and visualized through coding. His emphasis is on the flexibility and power of the process, ensuring that the visualization tools can handle a wide variety of data sets and analytical needs. While Tufte’s work could be seen as more static—refining the end product to perfection—Wickham’s is dynamic, enabling users to experiment, iterate, and scale visualizations across datasets."
  },
  {
    "objectID": "assigment3.html",
    "href": "assigment3.html",
    "title": "Assigment3",
    "section": "",
    "text": "rm(list=ls()) # Clear environment oldpar &lt;- par() # save default graphical parameters if (!is.null(dev.list()[“RStudioGD”])) # Clear plot window dev.off(dev.list()[“RStudioGD”])\ncat(“\\014”) # Clear the Console gc() # Setting the parameter (3 rows by 2 cols) par(mfrow=c(3, 2))"
  },
  {
    "objectID": "assigment3.html#simple-version",
    "href": "assigment3.html#simple-version",
    "title": "Assigment3",
    "section": "Simple version",
    "text": "Simple version\nplot(anscombe\\(x1,anscombe\\)y1) summary(anscombe)"
  },
  {
    "objectID": "assigment3.html#download-covid-data-from-owid-github",
    "href": "assigment3.html#download-covid-data-from-owid-github",
    "title": "Assigment3",
    "section": "Download COVID data from OWID GitHub",
    "text": "Download COVID data from OWID GitHub\nowidall = read.csv(“https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true”)) # Deselect cases/rows with OWID owidall = owidall[!grepl(”^OWID”, owidall$iso_code), ] # Subset by continent: Europe owideu = subset(owidall, continent==“Europe” & as.Date(date) &lt; as.Date(“2023-08-31”) & as.Date(date) &gt; as.Date(“2020-01-01”))\nsummary(owideu) ?plot\n# ylim = c(0,7000)\npar(family = “serif”)\nplot(x = as.Date(owideu\\(date), y = owideu\\)new_deaths, pch=16, col = “#bf106d”, xaxt=“n”, ylab=““, xlab =”“) axis.Date(1,at=as.Date(owideu\\(date),labels=format(as.Date(owideu\\)date),”%Y-%m”), las = 2, cex.axis = .8) mtext(“Date”, side=1, line=4, cex=2) mtext(“COVID Deaths in Europe (Daily)”, side=2, line=2, las=0, cex=1.5)"
  },
  {
    "objectID": "hackathon1.html",
    "href": "hackathon1.html",
    "title": "Hackathon1",
    "section": "",
    "text": "##Assignment4Compiled\ninstall.packages(“tidyverse”) library(tidyverse) library(dplyr) library(ggplot2)\nfile_path &lt;- “C:\\Users\\dava8\\OneDrive - The University of Texas at Dallas\\Notes\\Data viz UTD\\Assigments\\DV_ProjectData.csv” # or “C:\\Users\\ams190002\\Downloads\\DV_ProjectData.csv” data &lt;- read.csv(file_path) head(data) summary(data)"
  },
  {
    "objectID": "hackathon1.html#cleaning-data",
    "href": "hackathon1.html#cleaning-data",
    "title": "Hackathon1",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nstate_means &lt;- data %&gt;% group_by(State) %&gt;% summarise(across(c(VoterTurnoutRate, MedianHouseholdIncome, UnemploymentRate, HSCompletion, SomeCollegeEducation,PoorOrFairHealth, AvgPoorPhysHealthDays, AvgPoorMentalHealthDays, ObesityRate, FoodEnviornmentIndex, FoodInsecurity, LimitedAccesstoHealthyFoods, PhysInactivityRate,Population), mean)) View(state_means) states_of_interest &lt;- c(“CA”, “AR”, “TX”, “OK”, “NJ”) state_means_filtered &lt;- state_means %&gt;% filter(State %in% states_of_interest)\npar(family=“serif”)"
  },
  {
    "objectID": "hackathon1.html#graph-1",
    "href": "hackathon1.html#graph-1",
    "title": "Hackathon1",
    "section": "Graph 1",
    "text": "Graph 1\nbluecolorramp &lt;- colorRampPalette(c(“skyblue”, “mediumblue”))(length(state_means_filtered\\(MedianHouseholdIncome))\ncolors &lt;- bluecolorramp[rank(state_means_filtered\\)MedianHouseholdIncome)]\npar(family=“serif”, cex=0.9, mar=c(3, 3.5, 3, 1)) barplot(state_means_filtered\\(VoterTurnoutRate, width=state_means_filtered\\)MedianHouseholdIncome, space=0, col=colors, ylim=c(0,0.8), names.arg=state_means_filtered$State) title(main=“Voter Turnout vs Median Household Income”, cex.main=1.5) mtext(“Voter Turnout Rate”, , side=2, line=2.2, cex=1.1) box(bty=“l”) legend(“topright”, legend = c(“Low Income”, “High Income”), fill = c(“skyblue”, “mediumblue”), title = “Income Level”)"
  },
  {
    "objectID": "hackathon1.html#graph-2",
    "href": "hackathon1.html#graph-2",
    "title": "Hackathon1",
    "section": "Graph 2",
    "text": "Graph 2\nreduceddata &lt;- data%&gt;% filter(State == “CA”| State==“AR”|State ==“TX” | State == “OK”|State ==“NJ”)\nggplot(data = reduceddata,aes(x=VoterTurnoutRate, color = State))+ geom_density()+ scale_color_brewer(palette = “Dark2”)+ theme_minimal()+ theme(legend.position = “none”, text = element_text(family = “serif”))+ facet_wrap(~State)"
  },
  {
    "objectID": "hackathon1.html#graph-3",
    "href": "hackathon1.html#graph-3",
    "title": "Hackathon1",
    "section": "Graph 3",
    "text": "Graph 3\nggplot(state_means_filtered, aes(x = reorder(State, VoterTurnoutRate), y = VoterTurnoutRate, fill= “forestgreen”)) + geom_col(show.legend=FALSE) + labs(title = “Voter Turnout Rate by State”, x = “State”, y = “Voter Turnout Rate”) + theme_classic() + theme(text = element_text(family = “serif”)) + coord_flip()\n\n##Graph 4"
  }
]